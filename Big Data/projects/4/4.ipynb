{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Picked up _JAVA_OPTIONS: \n",
      "Picked up _JAVA_OPTIONS: \n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/spark3/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/06 19:40:24 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "SPARK_HOME = \"/usr/lib/spark3\"\n",
    "PYSPARK_PYTHON = \"/opt/conda/envs/dsenv/bin/python\"\n",
    "os.environ[\"PYSPARK_PYTHON\"]= PYSPARK_PYTHON\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]= PYSPARK_PYTHON\n",
    "os.environ[\"SPARK_HOME\"] = SPARK_HOME\n",
    "\n",
    "PYSPARK_HOME = os.path.join(SPARK_HOME, \"python/lib\")\n",
    "sys.path.insert(0, os.path.join(PYSPARK_HOME, \"py4j-0.10.9.5-src.zip\"))\n",
    "sys.path.insert(0, os.path.join(PYSPARK_HOME, \"pyspark.zip\"))\n",
    "\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.ui.port\", \"4321\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).appName(\"dont kill me\").getOrCreate()\n",
    "\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/datasets/amazon/train.json'\n",
    "test_path = '/datasets/amazon/test83m.json'\n",
    "# load sample of train path using spark:\n",
    "schema = StructType(fields=[\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"overall\", IntegerType()),\n",
    "    StructField(\"vote\", StringType()),\n",
    "    StructField(\"verified\", BooleanType()),\n",
    "    StructField(\"reviewTime\", StringType()),\n",
    "    StructField(\"reviewerID\", StringType()),\n",
    "    StructField(\"asin\", StringType()),\n",
    "    StructField(\"reviewerName\", StringType()),\n",
    "    StructField(\"reviewText\", StringType()),\n",
    "    StructField(\"summary\", StringType()),\n",
    "    StructField(\"unixReviewTime\", LongType()),\n",
    "])\n",
    "df_train = spark.read.json(train_path, schema=schema).sample(0.001)\n",
    "df_test = spark.read.json(test_path, schema=schema).sample(0.0001)\n",
    "# df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mcache()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+--------+-----------+--------------+----------+--------------+--------------------+--------------------+--------------+\n",
      "|   id|overall|vote|verified| reviewTime|    reviewerID|      asin|  reviewerName|          reviewText|             summary|unixReviewTime|\n",
      "+-----+-------+----+--------+-----------+--------------+----------+--------------+--------------------+--------------------+--------------+\n",
      "| 1149|   null|  11|   false| 03 1, 2017| A8AE8LXG37B33|B017OJL1IE|   M. Anderson|Even though the A...|Works great on my...|    1488326400|\n",
      "| 7876|   null|   6|   false| 12 1, 2016| A7E2C1E86ADAN|B01F9467IK|Matt Silverman|Love that it inte...|        Works great!|    1480550400|\n",
      "|56676|   null|null|   false| 05 7, 2010|A3UJ9MJGFGHFNE|B000PKKAFK| Adamcommenter|Let me start by s...|Disappointing for...|    1273190400|\n",
      "|58756|   null|null|    true|05 26, 2013| AZD3ON9ZMEGL6|B000URXP6E| huangweixiong|It smells good, s...|           i love it|    1369526400|\n",
      "|59491|   null|null|    true|11 20, 2017|A247EKUMNHZK9C|B000VV1YOY|        Tayler|Excellent price, ...|      Great product.|    1511136000|\n",
      "+-----+-------+----+--------+-----------+--------------+----------+--------------+--------------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"serve-DataFrame\" java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.net.PlainSocketImpl.socketAccept(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\n",
      "\tat java.net.ServerSocket.implAccept(ServerSocket.java:560)\n",
      "\tat java.net.ServerSocket.accept(ServerSocket.java:528)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:64)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_train.withColumn(\"text\", f.concat(f.col(\"reviewerName\"), f.lit(\". \"), f.col(\"reviewText\"), f.lit(\". \"), f.col(\"summary\"))).dropna(subset=[\"text\"])\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "train = tokenizer.transform(train)\n",
    "hasher = HashingTF(numFeatures=100, binary=True, inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "train = hasher.transform(train)\n",
    "train = train.withColumnRenamed(\"overall\", \"label\").select(\"id\", \"label\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_test.withColumn(\"text\", f.concat(f.col(\"reviewerName\"), f.lit(\". \"), f.col(\"reviewText\"), f.lit(\". \"), f.col(\"summary\"))).fillna(subset=[\"text\"], value=\"\")\n",
    "test = tokenizer.transform(test)\n",
    "test = hasher.transform(test)\n",
    "test = test.withColumnRenamed(\"overall\", \"label\").select(\"id\",\"label\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\", regParam=0.3)\n",
    "model = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test.limit(10))\n",
    "predictions = predictions.withColumn(\"prediction\", f.when(f.col(\"prediction\") > 5, 5).otherwise(f.when(f.col(\"prediction\") < 1, 1).otherwise(f.col(\"prediction\"))))\n",
    "\n",
    "# predictions.filter((f.col(\"prediction\") > 5) | (f.col(\"prediction\") < 1)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:=====================================================>  (69 + 3) / 72]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+--------------------+-----------------+\n",
      "|  id|label|            features|       prediction|\n",
      "+----+-----+--------------------+-----------------+\n",
      "|   5|    1|(100,[5,17,26,27,...|4.281522231182959|\n",
      "|4702|    5|(100,[3,5,6,9,12,...|4.121370756065989|\n",
      "|5017|    1|(100,[0,7,17,19,2...|4.239557252331887|\n",
      "|5442|    4|(100,[5,11,15,17,...|3.777200358720898|\n",
      "|5640|    5|(100,[3,8,9,11,14...|4.447536747278852|\n",
      "+----+-----+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml import Pipeline, Transformer\n",
    "\n",
    "# class ConcaterFillnaner_transformer(Transformer):\n",
    "#   def __init__(self): super(ConcaterFillnaner_transformer, self).__init__()\n",
    "#   def _transform(self, df):\n",
    "#     df = df_test.withColumn(\"text\", f.concat(f.col(\"reviewerName\"), f.lit(\". \"), f.col(\"reviewText\"), f.lit(\". \"), f.col(\"summary\"))).fillna(subset=[\"text\"], value=\"\")\n",
    "#     return df\n",
    "\n",
    "\n",
    "# class ConcaterFillnaner_estimator(Transformer):\n",
    "#   def __init__(self): super(ConcaterFillnaner_estimator, self).__init__()\n",
    "#   def _fit(self, df):\n",
    "#     df = df_train.withColumn(\"text\", f.concat(f.col(\"reviewerName\"), f.lit(\". \"), f.col(\"reviewText\"), f.lit(\". \"), f.col(\"summary\"))).dropna(subset=[\"text\"])\n",
    "#     return ConcaterFillnaner_transformer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "  tokenizer,\n",
    "  hasher,\n",
    "  lr\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot recognize a pipeline stage of type <class 'abc.ABCMeta'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(df_test)\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/spark3/python/lib/pyspark.zip/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/usr/lib/spark3/python/lib/pyspark.zip/pyspark/ml/pipeline.py:122\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m stage \u001b[38;5;129;01min\u001b[39;00m stages:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(stage, Estimator) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stage, Transformer)):\n\u001b[0;32m--> 122\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot recognize a pipeline stage of type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(stage))\n\u001b[1;32m    123\u001b[0m indexOfLastEstimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, stage \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(stages):\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot recognize a pipeline stage of type <class 'abc.ABCMeta'>."
     ]
    }
   ],
   "source": [
    "pipeline.fit(df_train).transform(df_test).show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
