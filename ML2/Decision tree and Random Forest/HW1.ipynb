{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0b76389",
   "metadata": {},
   "source": [
    "# Домашнее задание (10 баллов)\n",
    "\n",
    "1. (2 балла) Закончить реализацию `ClassificationDecisionTree` в decision_tree (реализовать feature_importance_, проверить корректность predict) и `RandomForestClassifier` в random_forest (predict/predict_proba). Обратите внимение, что в random_forest в качестве `base_estimator` предполагается использовать `DecisionTreeClassifier` из sklearn, использовать вашу реализацию решающего дерева необязательно. <br> Запуск тестов \n",
    "- `python -m unittest discover sem_dt_rf/decision_tree/tests`\n",
    "- `python -m unittest discover sem_dt_rf/random_forest/tests`\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bab43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m unittest discover sem_dt_rf/decision_tree/tests\n",
    "!python -m unittest discover sem_dt_rf/random_forest/tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78ab06d",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53eddec5",
   "metadata": {},
   "source": [
    "2. (1 балл) Для регрессионного дерева необходимо использовать такой критерий:\n",
    "    $$H(R) = \\min_c \\frac{1}{|R|} \\sum_{(x_i, y_i) \\in R} (y_i - c)^2$$\n",
    "    \n",
    "    Докажите, что минимум H(R) достигается при $c$:\n",
    "\n",
    "    $$ c = \\frac{1}{|R|} \\sum_{(x_j, y_j) \\in R} y_j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f61378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# док-во: правая часть H(R) - выпуклая ф-я по c. ноль производной (по c) этой ф-ии - как раз среднее."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0918bf7",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9489841",
   "metadata": {},
   "source": [
    "3. (3 балла) Реализуйте регрессионное дерево. В качестве критерия необходимо использовать критерий, определённый в пункте 2. В качестве функции выдачи результатов необходимо использовать среднее значение ответов по всем объектам в листе.\n",
    "\n",
    "    Сгенерируйте однопризнаковую выборку для тестирования дерева и покажите работу дерева на этой выборке (пример см. ниже, можно использовать свою версию). Отобразите на одном графике значения алгоритма и точки. Что меняется при изменении параметра глубины? Сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a498c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a0a5edb8764efeb80b37093be96d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='n', max=25, min=1), Output()), _dom_classes=('widget-in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from decision_tree.decision_tree import RegressionDecisionTree\n",
    "from ipywidgets import interact\n",
    "%matplotlib inline\n",
    "\n",
    "x_shape = 300\n",
    "depths_from = 1\n",
    "depths_to = 25\n",
    "\n",
    "# generate example\n",
    "x = np.arange(x_shape) / 100\n",
    "y = x**3 * np.sin(x**3) + np.random.random(x_shape)\n",
    "\n",
    "# store predictions for all depths\n",
    "predicts_depth = {}\n",
    "for n in range(depths_from, depths_to+1):\n",
    "  predicts_depth[n] = RegressionDecisionTree(max_depth=n, min_leaf_size=4).fit(x, y).predict(x)\n",
    "\n",
    "def update_plot(n=10):\n",
    "  fig, ax = plt.subplots(figsize=(16,8))\n",
    "  plt.scatter(x, y, color='g', label='true_values', s=10)\n",
    "  plt.scatter(x, predicts_depth[n], label='pred_values', alpha=0.8, s=20)\n",
    "  plt.title(f\"max_depth = {n}\")\n",
    "  plt.legend()\n",
    "  plt.xlabel(\"X\")\n",
    "  plt.ylabel(\"Y\")\n",
    "  plt.show()\n",
    "\n",
    "# plot slider\n",
    "interact(update_plot, n=(1,25,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f33403",
   "metadata": {},
   "source": [
    "- при маленькой глуине сплиты делаются на участках с самой большой дисперсией (максимизируют MSE..)\n",
    "- при увеличении глубины сплиты захватывают области с меньшей остаточной дисперсией (справа налево), bias модели, при этом, растет (тк в правой части имеем случайные выбросы, а лучше, наверное иметь просто прямую)\n",
    "- при максимальной глубине области с меньшей дисперсией хорошо выучились деревом, в то время как области с большой дисперсией имеют меньше сплитов и являются источником бед модели (там в принципе меньше точек-данных)\n",
    "\n",
    "вывод?\n",
    "- если хочется зафитится больше к данным -> повышаем глубину\n",
    "- если хотим иметь лучшую обобщающую способность -> уменьшаем глубину"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877574ca",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd31212",
   "metadata": {},
   "source": [
    "4. (4 балла) Протестируйте различные реализации `random_forest` на `fetch_covtype` датасете (можно загрузить с помощью `sklearn.datasets.fetch_covtype`). Возможно, поможет ноутбук с семинара `ensembles_seminar.ipynb`. Для честного сравнения старайтесь использовать похожий набор гиперпараметров.\n",
    "- ваша реализация (import `RandomForestClassifier as MyRandomForestClassifier` ниже)\n",
    "- sklearn https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "- lightgbm https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMModel.html см. параметр `boosting_type`\n",
    "- xgboost https://xgboost.readthedocs.io/en/stable/tutorials/rf.html\n",
    "\n",
    "    Что нужно сделать: \n",
    "- Разбейте данные на train и test. \n",
    "- Оцените качество алгоритмов по метрике (balanced_accuracy_score)[https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html]\n",
    "- Оцените время работы `train` и `predict`\n",
    "- Сделайте выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71b3a264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import sys\n",
    "# sys.path.extend(['/Users/alkrasnov/Documents/AM_ML_2_24']) # change your path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c3f2f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sem_dt_rf.random_forest.random_forest import RandomForestClassifier as MyRandomForestClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import xgboost  as xgb\n",
    "\n",
    "from sklearn.datasets import fetch_covtype\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "\n",
    "cov_type = fetch_covtype(data_home='_temp', download_if_missing=False)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "target = LabelEncoder().fit_transform(cov_type.target)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cov_type.data, target, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "071ad18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my model, training time: 121.57 s\n",
      "my model, predict time: 6.58 s\n",
      "my model, score: 0.50\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "my_rf = MyRandomForestClassifier(n_estimators=100, max_depth=8, min_samples_leaf=50)\n",
    "\n",
    "train_time = time.time()\n",
    "my_rf.fit(X_train, y_train)\n",
    "train_time = time.time()-train_time\n",
    "print(f'my model, training time: {train_time:.2f} s')\n",
    "\n",
    "predict_time = time.time()\n",
    "my_preds = my_rf.predict(X_test)\n",
    "predict_time = time.time()-predict_time\n",
    "print(f'my model, predict time: {predict_time:.2f} s')\n",
    "\n",
    "my_score = balanced_accuracy_score(y_test, my_preds)\n",
    "print(f'my model, score: {my_score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "38d1139e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn, training time: 22.38 s\n",
      "sklearn, predict time: 0.66 s\n",
      "sklearn, score: 0.40\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, max_depth=8, min_samples_leaf=50)\n",
    "\n",
    "train_time = time.time()\n",
    "rf.fit(X_train, y_train)\n",
    "train_time = time.time()-train_time\n",
    "print(f'sklearn, training time: {train_time:.2f} s')\n",
    "\n",
    "predict_time = time.time()\n",
    "rf_preds = rf.predict(X_test)\n",
    "predict_time = time.time()-predict_time\n",
    "print(f'sklearn, predict time: {predict_time:.2f} s')\n",
    "\n",
    "rf_score = balanced_accuracy_score(y_test, rf_preds)\n",
    "print(f'sklearn, score: {rf_score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "09a6f7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lgbm, training time: 14.53 s\n",
      "lgbm, predict time: 1.21 s\n",
      "lgbm, score: 0.80\n"
     ]
    }
   ],
   "source": [
    "lgbt_params = {\n",
    "    'num_leaves': 2**8,\n",
    "    'min_data_in_leaf': 50,\n",
    "    'objective': 'multiclass',\n",
    "    'num_classes': len(np.unique(target)),\n",
    "    'max_depth': 8,\n",
    "    \"boosting\": \"rf\",\n",
    "    \"seed\": 42,\n",
    "    \"bagging_frequency\": 0.65,\n",
    "    \"subsample\": .632,\n",
    "    \"subsample_freq\": 1,\n",
    "    \"verbose\": -1,\n",
    "    \"num_threads\": -1\n",
    "}\n",
    "\n",
    "train_time = time.time()\n",
    "lgbm = lgb.train(lgbt_params, lgb.Dataset(data=X_train, label=y_train), num_boost_round=100)\n",
    "train_time = time.time()-train_time\n",
    "print(f'lgbm, training time: {train_time:.2f} s')\n",
    "\n",
    "predict_time = time.time()\n",
    "lgbm_preds = lgbm.predict(X_test).argmax(axis=1)\n",
    "predict_time = time.time()-predict_time\n",
    "print(f'lgbm, predict time: {predict_time:.2f} s')\n",
    "\n",
    "lgbm_score = balanced_accuracy_score(y_test, lgbm_preds)\n",
    "print(f'lgbm, score: {lgbm_score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6dc1b683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lgbm, training time: 10.38 s\n",
      "lgbm, predict time: 0.42 s\n",
      "lgbm, score: 0.66\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"subsample\": .632,\n",
    "    \"max_depth\": 8,\n",
    "    \"num_parallel_tree\": 100,\n",
    "    \"objective\": \"multi:softmax\",\n",
    "    \"num_class\": len(np.unique(target)),\n",
    "    \"eta\":1,\n",
    "    \"random_state\": 42,\n",
    "    \"n_jobs\": -1,\n",
    "}\n",
    "\n",
    "train_time = time.time()\n",
    "xgbm = xgb.train(xgb_params, xgb.DMatrix(X_train, label=y_train), num_boost_round=1)\n",
    "train_time = time.time()-train_time\n",
    "print(f'lgbm, training time: {train_time:.2f} s')\n",
    "\n",
    "predict_time = time.time()\n",
    "xgbm_preds = xgbm.predict(xgb.DMatrix(X_test))\n",
    "predict_time = time.time()-predict_time\n",
    "print(f'lgbm, predict time: {predict_time:.2f} s')\n",
    "\n",
    "xgbm_score = balanced_accuracy_score(y_test, xgbm_preds)\n",
    "print(f'lgbm, score: {xgbm_score:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d2a313",
   "metadata": {},
   "source": [
    "- sklearn: худший скор!, при этом время работы дольше чем у xgb. Нужно тщательнее настраивать гиперпараметры (и не использовать эту модель)\n",
    "- наша модель: (работает на питоне) работает дольше всех, на дефолтных параметрах довольно скудный результат\n",
    "- lgb: лучший скор, не лучшая скорость трейн и инференс, но сравнимая с xgb\n",
    "- xgb: лучшая скорость трейн и инференс, но скор несравнимо ниже чем у lgb\n",
    "\n",
    "выводы..\n",
    "- лучше использовать lightgbm (либо CatBoost, это пока загадка)\n",
    "- на совсем огромных датасетах, возможно, еще можно рассмотреть xgb (но, скорее всего, в lgb можно подобрать параметры для нужной скорости с сохранением скора)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
